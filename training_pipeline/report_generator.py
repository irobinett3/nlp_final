#!/usr/bin/env python3
# -*- coding: utf-8 -*-
# report_generator.py

"""Generate comprehensive training data reports"""

import os
from datetime import datetime
from typing import Dict, Optional

from ground_truth_events import GROUND_TRUTH_EVENTS


def generate_report(datasets: Dict, output_dir: Optional[str] = None) -> str:
    """Generate comprehensive markdown report
    
    Args:
        datasets: Dictionary containing all dataset splits
        output_dir: Optional output directory. If None, uses current directory
    """
    
    # Use provided output_dir or current directory
    if output_dir is None:
        from .config import OUTPUT_DIR
        output_dir = OUTPUT_DIR
    
    clf = datasets['classification']
    bio = datasets['span_detection']
    llm = datasets.get('llm_baseline', {})
    attr = datasets.get('attributions', [])
    
    # Calculate statistics
    gt_count = sum(1 for d in clf['train'] if d.get('labels', {}).get('ground_truth'))
    perfect_agreement = sum(
        1 for d in clf['train'] 
        if d.get('labels', {}).get('agreement_level') == 3
    )
    
    report = f"""# Risk Detection Training Data - Comprehensive Report

Generated: {datetime.now().isoformat()}

## Dataset Statistics

### Classification Dataset
- **Training**: {len(clf['train'])} examples
  - Ground truth from historical events: {gt_count}
  - Perfect 3-source agreement: {perfect_agreement}
- **Validation**: {len(clf['val'])} examples
- **Test**: {len(clf['test'])} examples

### Span Detection Dataset
- **Training**: {len(bio['train'])} documents
  - Total sentences: {sum(d.get('bio_tags', {}).get('total_sentences', 0) for d in bio['train'])}
- **Validation**: {len(bio['val'])} documents
- **Test**: {len(bio['test'])} documents

### Attribution Data
- **Documents with attributions**: {len(attr)}
- **Total risk spans extracted**: {sum(len(a.get('risk_spans', [])) for a in attr)}

### LLM Baseline
"""
    
    if llm.get('best_configuration'):
        best = llm['best_configuration']
        report += f"""- **Best Model**: {best.get('model', 'N/A')}
- **Best Strategy**: {best.get('strategy', 'N/A')}
- **F1 Score**: {best.get('f1', 0):.3f}
- **Cost per Document**: ${best.get('cost_per_doc', 0):.4f}
"""
    else:
        report += "- LLM evaluation not performed\n"
    
    report += f"""
## Historical Ground Truth Events

This dataset includes {len(GROUND_TRUTH_EVENTS)} historical corporate risk events:

"""
    
    # Sample events
    for event in GROUND_TRUTH_EVENTS[:5]:
        report += f"- **{event['company']}** ({event.get('ticker', 'N/A')}): {event['event']}\n"
        report += f"  - Severity: {event['severity']}\n"
        report += f"  - Types: {', '.join(event['risk_type'])}\n\n"
    
    report += f"""
## Recommended Next Steps

1. **Train Classification Model**
   - Use `classification/train.jsonl` to fine-tune BERT/FinBERT
   - Expected F1: 0.75-0.85 with sufficient data
   
2. **Train Span Detection Model**
   - Use `span_detection/train.conll` to fine-tune token classifier
   - Enables granular risk highlighting
   
3. **Implement Attribution Visualization**
   - Use `attributions.json` for instant risk highlighting
   - No additional model training needed
   
4. **Compare with LLM Baseline**
   - Fine-tuned model should be faster and cheaper
   - LLM provides strong baseline for validation

## Files Generated

- `classification/train.jsonl` - Training data for risk classification
- `classification/val.jsonl` - Validation data
- `classification/test.jsonl` - Test data
- `span_detection/train.conll` - BIO-tagged data for span detection
- `span_detection/val.conll` - Validation spans
- `span_detection/test.conll` - Test spans
- `llm_baseline_results.json` - LLM evaluation results
- `attributions.json` - Token attributions for explainability
- `training_report.md` - This report

## Data Quality Notes

- High-quality labels have 2-3 source agreement (confidence â‰¥ {0.67})
- Ground truth examples have perfect confidence (we know the outcome)
- BIO tags combine pattern matching + FinBERT attribution
- Attribution scores enable explainable risk detection without labels

---

*Generated by risk_training_pipeline.py*
"""
    
    # Save report
    filepath = os.path.join(output_dir, 'training_report.md')
    with open(filepath, 'w') as f:
        f.write(report)
    
    print(f"[Report] Saved to {filepath}")
    
    return report